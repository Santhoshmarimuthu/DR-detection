{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVzeOwgcibR9",
        "outputId": "7f02c177-6159-4dce-994e-d80a3dde4b8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'sudo' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'sudo' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install python3.10-venv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kjUnUI6kh93Y"
      },
      "outputs": [],
      "source": [
        "!python -m venv myenv\n",
        "!source myenv/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY_nkpS4FzTY",
        "outputId": "b99f6c0f-66f4-435e-b62a-116553a4466e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'causal-conv1d-1'...\n",
            "remote: Enumerating objects: 445, done.\u001b[K\n",
            "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
            "remote: Total 445 (delta 150), reused 155 (delta 112), pack-reused 226 (from 1)\u001b[K\n",
            "Receiving objects: 100% (445/445), 105.44 KiB | 6.20 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Dao-AILab/causal-conv1d.git causal-conv1d-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAABufM1jiQT",
        "outputId": "8876e306-3994-4ba4-ddf2-0045dff3e761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/causal-conv1d-1/setup.py\n"
          ]
        }
      ],
      "source": [
        "!ls /content/causal-conv1d-1/setup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Brwn1Hjs7S",
        "outputId": "c909325e-4f70-4ed6-e7f1-5d2b62cc8553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/causal-conv1d-1\n"
          ]
        }
      ],
      "source": [
        "cd /content/causal-conv1d-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nwPw8FzgsRg",
        "outputId": "8c298d5e-0fd2-4af7-9cf8-eb6fdf4f937d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/causal-conv1d-1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal_conv1d==1.5.0.post8) (2.5.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal_conv1d==1.5.0.post8) (24.2)\n",
            "Collecting ninja (from causal_conv1d==1.5.0.post8)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.5.0.post8) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.5.0.post8) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.5.0.post8) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.5.0.post8) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.5.0.post8) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal_conv1d==1.5.0.post8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->causal_conv1d==1.5.0.post8) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->causal_conv1d==1.5.0.post8) (3.0.2)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: causal_conv1d\n",
            "  Building wheel for causal_conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal_conv1d: filename=causal_conv1d-1.5.0.post8-cp310-cp310-linux_x86_64.whl size=103960374 sha256=ec956a2d0fa48bd16a8dd5be9ad1c03db88565ea6ee2679e362940fef659284b\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/2a/e4/41af7be69d83d2226d4f63cf19e1af2f62aebbf141ff03521e\n",
            "Successfully built causal_conv1d\n",
            "Installing collected packages: ninja, causal_conv1d\n",
            "Successfully installed causal_conv1d-1.5.0.post8 ninja-1.11.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFezXXyU-909",
        "outputId": "642e1f0b-86c9-4561-ce51-7dbe5b1ffd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting causal_conv1d==1.0.0\n",
            "  Using cached causal_conv1d-1.0.0.tar.gz (6.4 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install causal_conv1d==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mGOJ8UgFlmI",
        "outputId": "30a9fd2c-4899-4b40-8783-abdc6b98edfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A--1ywQ9k2q9",
        "outputId": "30686806-872a-4b6c-eeb8-5a82c0d34ab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "zS_BNnACz7JK",
        "outputId": "e4f5bd0a-c8aa-49ff-ac37-833340de3a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.0+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.0%2Bcu117-cp310-cp310-linux_x86_64.whl (1806.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.0+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.0%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.0+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.0%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0+cu117) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0+cu117) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0+cu117) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0+cu117) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117) (2024.12.14)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "Successfully installed torch-1.13.0+cu117 torchaudio-0.13.0+cu117 torchvision-0.14.0+cu117\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b96ae16e84eb40e7bfffc7b5912003e2",
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio==0.13.0+cu117 --index-url https://download.pytorch.org/whl/cu117\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWA5x56FEpxh",
        "outputId": "25686e11-cfa7-44be-cec9-359e9d6b9d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11.7\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXKM7xENBGG6",
        "outputId": "a3c5b62a-9083-4912-9be9-038a1055e367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.13.0+cu117 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (1.13.0+cu117)\n",
            "Requirement already satisfied: torchvision==0.14.0+cu117 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (0.14.0+cu117)\n",
            "Requirement already satisfied: torchaudio==0.13.0+cu117 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 3)) (0.13.0+cu117)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (24.2)\n",
            "Collecting timm==0.4.12 (from -r /content/requirements.txt (line 5))\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 6)) (8.3.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 7)) (5.2.0)\n",
            "Collecting yacs (from -r /content/requirements.txt (line 8))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 9)) (2.5.0)\n",
            "Collecting submitit (from -r /content/requirements.txt (line 10))\n",
            "  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting tensorboardX (from -r /content/requirements.txt (line 11))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting triton==2.0.0 (from -r /content/requirements.txt (line 12))\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting causal_conv1d==1.0.0 (from -r /content/requirements.txt (line 13))\n",
            "  Downloading causal_conv1d-1.0.0.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mamba_ssm==1.0.1 (from -r /content/requirements.txt (line 14))\n",
            "  Downloading mamba_ssm-1.0.1.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 15)) (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 16)) (3.8.0)\n",
            "Collecting thop (from -r /content/requirements.txt (line 17))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 18)) (3.12.1)\n",
            "Collecting SimpleITK (from -r /content/requirements.txt (line 19))\n",
            "  Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 20)) (0.25.0)\n",
            "Collecting medpy (from -r /content/requirements.txt (line 21))\n",
            "  Downloading medpy-0.5.2.tar.gz (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0+cu117->-r /content/requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0+cu117->-r /content/requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0+cu117->-r /content/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.0+cu117->-r /content/requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->-r /content/requirements.txt (line 12)) (3.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->-r /content/requirements.txt (line 12)) (3.16.1)\n",
            "Collecting lit (from triton==2.0.0->-r /content/requirements.txt (line 12))\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ninja (from causal_conv1d==1.0.0->-r /content/requirements.txt (line 13))\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (4.47.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->-r /content/requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->-r /content/requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->-r /content/requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->-r /content/requirements.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->-r /content/requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r /content/requirements.txt (line 10)) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r /content/requirements.txt (line 11)) (4.25.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /content/requirements.txt (line 15)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /content/requirements.txt (line 15)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r /content/requirements.txt (line 15)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 16)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 16)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 16)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 16)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 16)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/requirements.txt (line 20)) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/requirements.txt (line 20)) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/requirements.txt (line 20)) (2024.12.12)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/requirements.txt (line 20)) (0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r /content/requirements.txt (line 16)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117->-r /content/requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117->-r /content/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117->-r /content/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.0+cu117->-r /content/requirements.txt (line 2)) (2024.12.14)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (0.27.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers->mamba_ssm==1.0.1->-r /content/requirements.txt (line 14)) (2024.10.0)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading submitit-1.5.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: causal_conv1d, mamba_ssm, medpy\n",
            "  Building wheel for causal_conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal_conv1d: filename=causal_conv1d-1.0.0-cp310-cp310-linux_x86_64.whl size=8965723 sha256=2f3c2b5707b38fecdc6021ebf73e07d669127266fe7c8d1e46e536727bcedafc\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/48/f5/eb0c6d6d8e00131eaa57927b537a23832b37e2f01b801d9c5d\n",
            "  Building wheel for mamba_ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba_ssm: filename=mamba_ssm-1.0.1-cp310-cp310-linux_x86_64.whl size=152100816 sha256=70fdf9edb1fe07e07efa07f1363218cb696ed7a626dd917018ec6b3cdb33f17b\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/cf/65/cc589985f9689241fe2c154ce1c60738f58a24e76ce474cc20\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.5.2-cp310-cp310-linux_x86_64.whl size=762836 sha256=04206199016ce7c0eb8fa970fd11bbc45617d67be835a3a2c99a051296511af8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b8/63/bdf557940ec60d1b8822e73ff9fbe7727ac19f009d46b5d175\n",
            "Successfully built causal_conv1d mamba_ssm medpy\n",
            "Installing collected packages: SimpleITK, lit, yacs, tensorboardX, submitit, ninja, triton, thop, medpy, causal_conv1d, timm, mamba_ssm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.12\n",
            "    Uninstalling timm-1.0.12:\n",
            "      Successfully uninstalled timm-1.0.12\n",
            "Successfully installed SimpleITK-2.4.0 causal_conv1d-1.0.0 lit-18.1.8 mamba_ssm-1.0.1 medpy-0.5.2 ninja-1.11.1.3 submitit-1.5.2 tensorboardX-2.6.2.2 thop-0.1.1.post2209072238 timm-0.4.12 triton-2.0.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LqiDYi4ODbn_"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/archive (1).zip', 'r') # replace with your zip file path\n",
        "zip_ref.extractall('/content/destination_folder') # replace with the path to extract to\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qtwXG6CIZ2-",
        "outputId": "152788f6-fdf0-4b45-824e-d42ce19630e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'MedMamba'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 127 (delta 53), reused 90 (delta 28), pack-reused 4 (from 1)\u001b[K\n",
            "Receiving objects: 100% (127/127), 353.24 KiB | 27.17 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "torch.Size([1, 6])\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/YubiaoYue/MedMamba.git\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/MedMamba')  # Assuming you cloned to /content/\n",
        "\n",
        "import MedMamba # Import the VSSM class\n",
        "\n",
        "# ... (your code to use the MedMamba model) ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8I9Tuj4LMwhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/MedMamba')  # Replace 'repo_name' with your repo folder name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fbftBtCFEPr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdHvEDazFlfV",
        "outputId": "0f54c2ca-439c-4926-c026-97d59ccf9fdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 6])\n",
            "Using cuda:0 device.\n",
            "Using 2 dataloader workers every process\n",
            "Using 3656 images for training, 3656 images for validation.\n",
            "train epoch[1/100] loss:0.997: 100%|██████████| 115/115 [01:39<00:00,  1.15it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.34it/s]\n",
            "[epoch 1] train_loss: 1.159  val_accuracy: 0.621\n",
            "train epoch[2/100] loss:1.289: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 2] train_loss: 1.007  val_accuracy: 0.685\n",
            "train epoch[3/100] loss:0.688: 100%|██████████| 115/115 [01:39<00:00,  1.15it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.37it/s]\n",
            "[epoch 3] train_loss: 0.957  val_accuracy: 0.673\n",
            "train epoch[4/100] loss:1.299: 100%|██████████| 115/115 [01:39<00:00,  1.15it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.32it/s]\n",
            "[epoch 4] train_loss: 0.905  val_accuracy: 0.718\n",
            "train epoch[5/100] loss:0.663: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
            "[epoch 5] train_loss: 0.902  val_accuracy: 0.718\n",
            "train epoch[6/100] loss:1.179: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
            "[epoch 6] train_loss: 0.894  val_accuracy: 0.708\n",
            "train epoch[7/100] loss:1.042: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 7] train_loss: 0.871  val_accuracy: 0.660\n",
            "train epoch[8/100] loss:1.598: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.33it/s]\n",
            "[epoch 8] train_loss: 0.845  val_accuracy: 0.709\n",
            "train epoch[9/100] loss:0.747: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.35it/s]\n",
            "[epoch 9] train_loss: 0.839  val_accuracy: 0.720\n",
            "train epoch[10/100] loss:1.710: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.37it/s]\n",
            "[epoch 10] train_loss: 0.816  val_accuracy: 0.712\n",
            "train epoch[11/100] loss:1.165: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 11] train_loss: 0.812  val_accuracy: 0.705\n",
            "train epoch[12/100] loss:0.610: 100%|██████████| 115/115 [01:39<00:00,  1.15it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 12] train_loss: 0.807  val_accuracy: 0.720\n",
            "train epoch[13/100] loss:0.774: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.35it/s]\n",
            "[epoch 13] train_loss: 0.774  val_accuracy: 0.731\n",
            "train epoch[14/100] loss:0.347: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
            "[epoch 14] train_loss: 0.771  val_accuracy: 0.687\n",
            "train epoch[15/100] loss:0.728: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 15] train_loss: 0.762  val_accuracy: 0.711\n",
            "train epoch[16/100] loss:0.426: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.40it/s]\n",
            "[epoch 16] train_loss: 0.775  val_accuracy: 0.724\n",
            "train epoch[17/100] loss:0.232: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.32it/s]\n",
            "[epoch 17] train_loss: 0.724  val_accuracy: 0.715\n",
            "train epoch[18/100] loss:1.164: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
            "[epoch 18] train_loss: 0.737  val_accuracy: 0.739\n",
            "train epoch[19/100] loss:1.020: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
            "[epoch 19] train_loss: 0.722  val_accuracy: 0.740\n",
            "train epoch[20/100] loss:0.405: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 20] train_loss: 0.727  val_accuracy: 0.759\n",
            "train epoch[21/100] loss:0.819: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.31it/s]\n",
            "[epoch 21] train_loss: 0.702  val_accuracy: 0.759\n",
            "train epoch[22/100] loss:0.444: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.37it/s]\n",
            "[epoch 22] train_loss: 0.689  val_accuracy: 0.732\n",
            "train epoch[23/100] loss:0.422: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
            "[epoch 23] train_loss: 0.720  val_accuracy: 0.761\n",
            "train epoch[24/100] loss:0.745: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 24] train_loss: 0.687  val_accuracy: 0.741\n",
            "train epoch[25/100] loss:0.713: 100%|██████████| 115/115 [01:39<00:00,  1.15it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.35it/s]\n",
            "[epoch 25] train_loss: 0.696  val_accuracy: 0.753\n",
            "train epoch[26/100] loss:0.393: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.35it/s]\n",
            "[epoch 26] train_loss: 0.683  val_accuracy: 0.772\n",
            "train epoch[27/100] loss:0.566: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.36it/s]\n",
            "[epoch 27] train_loss: 0.671  val_accuracy: 0.754\n",
            "train epoch[28/100] loss:0.559: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.37it/s]\n",
            "[epoch 28] train_loss: 0.678  val_accuracy: 0.771\n",
            "train epoch[29/100] loss:1.381: 100%|██████████| 115/115 [01:39<00:00,  1.15it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.35it/s]\n",
            "[epoch 29] train_loss: 0.669  val_accuracy: 0.784\n",
            "train epoch[30/100] loss:0.521: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.35it/s]\n",
            "[epoch 30] train_loss: 0.661  val_accuracy: 0.790\n",
            "train epoch[31/100] loss:1.113: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.37it/s]\n",
            "[epoch 31] train_loss: 0.654  val_accuracy: 0.792\n",
            "train epoch[32/100] loss:0.757: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 32] train_loss: 0.652  val_accuracy: 0.783\n",
            "train epoch[33/100] loss:1.446: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 33] train_loss: 0.631  val_accuracy: 0.790\n",
            "train epoch[34/100] loss:0.748: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.34it/s]\n",
            "[epoch 34] train_loss: 0.633  val_accuracy: 0.790\n",
            "train epoch[35/100] loss:0.791: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.35it/s]\n",
            "[epoch 35] train_loss: 0.632  val_accuracy: 0.787\n",
            "train epoch[36/100] loss:0.511: 100%|██████████| 115/115 [01:39<00:00,  1.16it/s]\n",
            "100%|██████████| 115/115 [00:26<00:00,  4.38it/s]\n",
            "[epoch 36] train_loss: 0.628  val_accuracy: 0.787\n",
            "Early stopping triggered!\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import importlib\n",
        "import MedMamba\n",
        "importlib.reload(MedMamba)\n",
        "from MedMamba import VSSM\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, delta=0):\n",
        "        self.patience = patience  # How many epochs to wait before stopping\n",
        "        self.delta = delta  # Minimum change to qualify as an improvement\n",
        "        self.best_val_accuracy = 0.0\n",
        "        self.counter = 0\n",
        "\n",
        "    def check_early_stop(self, val_accuracy):\n",
        "        if val_accuracy > self.best_val_accuracy + self.delta:\n",
        "            self.best_val_accuracy = val_accuracy\n",
        "            self.counter = 0  # Reset counter\n",
        "            return False  # No early stop\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True  # Stop training\n",
        "            return False  # No early stop\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using {} device.\".format(device))\n",
        "\n",
        "\n",
        "    data_transform = {\n",
        "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                     transforms.RandomHorizontalFlip(),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "        \"val\": transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(root=r\"/content/destination_folder/colored_images\",\n",
        "                                         transform=data_transform[\"train\"])\n",
        "    train_num = len(train_dataset)\n",
        "\n",
        "    flower_list = train_dataset.class_to_idx\n",
        "    cla_dict = dict((val, key) for key, val in flower_list.items())\n",
        "    # write dict into json file\n",
        "    json_str = json.dumps(cla_dict, indent=4)\n",
        "    with open('class_indices.json', 'w') as json_file:\n",
        "        json_file.write(json_str)\n",
        "\n",
        "    batch_size = 32\n",
        "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
        "    print('Using {} dataloader workers every process'.format(nw))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=batch_size, shuffle=True,\n",
        "                                               num_workers=nw)\n",
        "\n",
        "    validate_dataset = datasets.ImageFolder(root=r\"/content/destination_folder/colored_images\",\n",
        "                                            transform=data_transform[\"val\"])\n",
        "    val_num = len(validate_dataset)\n",
        "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
        "                                                  batch_size=batch_size, shuffle=False,\n",
        "                                                  num_workers=nw)\n",
        "    print(\"Using {} images for training, {} images for validation.\".format(train_num,\n",
        "                                                                           val_num))\n",
        "\n",
        "    net = VSSM(num_classes=5)\n",
        "    net.to(device)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "    epochs = 100\n",
        "    best_acc = 0.0\n",
        "    model_name = \"MedMamba\"\n",
        "    save_path = './{}Net.pth'.format(model_name)\n",
        "    train_steps = len(train_loader)\n",
        "\n",
        "    # Initialize EarlyStopping\n",
        "    early_stopping = EarlyStopping(patience=5, delta=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # train\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        for step, data in enumerate(train_bar):\n",
        "            images, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images.to(device))\n",
        "            loss = loss_function(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
        "                                                                     epochs,\n",
        "                                                                     loss.item())\n",
        "\n",
        "        # validate\n",
        "        net.eval()\n",
        "        acc = 0.0  # accumulate accurate number / epoch\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(validate_loader, file=sys.stdout)\n",
        "            for val_data in val_bar:\n",
        "                val_images, val_labels = val_data\n",
        "                outputs = net(val_images.to(device))\n",
        "                predict_y = torch.max(outputs, dim=1)[1]\n",
        "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
        "\n",
        "        val_accuracy = acc / val_num\n",
        "        print(f'[epoch {epoch + 1}] train_loss: {running_loss / train_steps:.3f}  val_accuracy: {val_accuracy:.3f}')\n",
        "\n",
        "        # Check for early stopping\n",
        "        if early_stopping.check_early_stop(val_accuracy):\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break  # Stops training early\n",
        "\n",
        "        # Save the model if validation accuracy is improved\n",
        "        if val_accuracy > best_acc:\n",
        "            best_acc = val_accuracy\n",
        "            torch.save(net.state_dict(), save_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdpYOxPgpzjy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
